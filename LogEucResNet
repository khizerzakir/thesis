{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8207310,"sourceType":"datasetVersion","datasetId":4863207},{"sourceId":8386035,"sourceType":"datasetVersion","datasetId":4987693},{"sourceId":176224317,"sourceType":"kernelVersion"},{"sourceId":178092239,"sourceType":"kernelVersion"},{"sourceId":178092775,"sourceType":"kernelVersion"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport subprocess\nimport sys\nsys.path.insert(0,'..')\nfrom pathlib import Path\nimport os\nimport random\n\nimport matplotlib.pylab as plt\n\nimport time\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nfrom torch.utils import data\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, WeightedRandomSampler\n\n\nfrom focal_loss import FocalLoss\nfrom utils import _axat, _atxa, _mvmt\nfrom bsclassifier import BusemanSimilarityClassifier\nimport seaborn as sns\n\n\ndef install(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package],\n                         stdout=subprocess.DEVNULL,\n                         stderr=subprocess.DEVNULL\n                         )\nrequired_packages = [\n    \"geoopt\",\n    \"pyriemann\",\n]\nfor package in required_packages:\n    try:\n        __import__(package)\n        print(f\"{package} is already installed.\")\n    except ImportError:\n        print(f\"{package} is not installed. Installing...\")\n        install(package)\n\nimport geoopt\nfrom pyriemann.utils.covariance import covariances, normalize\n\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.manifold import TSNE\nimport pandas as pd\n\n\nuse_cuda = th.cuda.is_available()\ndevice = th.device(\"cuda:0\" if use_cuda else \"cpu\")\nprint(device)","metadata":{"_uuid":"e5bceb58-dcde-4b0d-bc93-08fbbcd83f81","_cell_guid":"7a0be6c0-5063-4a20-863f-e6218822d820","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-16T23:54:07.775424Z","iopub.execute_input":"2024-05-16T23:54:07.775778Z","iopub.status.idle":"2024-05-16T23:54:36.650137Z","shell.execute_reply.started":"2024-05-16T23:54:07.775750Z","shell.execute_reply":"2024-05-16T23:54:36.649134Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting geoopt\n  Downloading geoopt-0.5.0-py3-none-any.whl.metadata (6.7 kB)\nRequirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from geoopt) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from geoopt) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from geoopt) (1.11.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->geoopt) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->geoopt) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->geoopt) (1.3.0)\nDownloading geoopt-0.5.0-py3-none-any.whl (90 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: geoopt\nSuccessfully installed geoopt-0.5.0\ngeoopt is already installed.\npyriemann is not installed. Installing...\ncuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"kwargs = {'num_workers': 8, 'pin_memory': True} if use_cuda else {}\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    th.manual_seed(seed)\n    th.cuda.manual_seed(seed)\n    th.cuda.manual_seed_all(seed)\n    th.backends.cudnn.benchmark = False\n    th.backends.cudnn.deterministic = True\n\nseed_torch()","metadata":{"_uuid":"4779ba09-bf19-43f0-a86b-d74db9acd78d","_cell_guid":"54dbdcd3-ca20-4ae0-95f1-ad47972f783d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-16T23:54:36.651674Z","iopub.execute_input":"2024-05-16T23:54:36.652090Z","iopub.status.idle":"2024-05-16T23:54:36.660532Z","shell.execute_reply.started":"2024-05-16T23:54:36.652065Z","shell.execute_reply":"2024-05-16T23:54:36.659821Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Model definition","metadata":{"_uuid":"8d24807d-0df5-490e-811a-3b4f0fb153b2","_cell_guid":"fa1dbad1-5099-4500-a958-c841390ead20","trusted":true}},{"cell_type":"code","source":"class LogEucRResNet(nn.Module):\n    def __init__(self, inputdim=10, dim1=7, n_proto=12, classes=11, embed_only=False):\n        super().__init__()\n        self.inputdim = inputdim\n        self.dim1 = dim1\n        self.n_proto = n_proto\n        self.classes = classes\n        self.embed_only = embed_only\n\n        self.manifold_Stiefel = geoopt.Stiefel()\n        self.manifold_SPD = geoopt.SymmetricPositiveDefinite(\"LEM\")\n        \n        # init first bimap\n        bm1 = th.randn((1, self.inputdim, self.dim1), dtype=th.float64)\n        bm1 = th.svd(bm1)[0]\n        self.register_parameter(\"bimap1\", geoopt.ManifoldParameter(bm1, self.manifold_Stiefel))\n        \n        P = th.randn((1, self.dim1, self.dim1), dtype=th.float64)\n        P = th.svd(P)[0]\n\n        self.register_parameter(\"P1\", geoopt.ManifoldParameter(P, self.manifold_Stiefel))\n\n        self.classiflayer=BusemanSimilarityClassifier(self.dim1,self.n_proto, classes)\n        self.fc_layer = nn.Linear(self.dim1 * self.dim1, self.classes).double()\n        self.softmax =  nn.Softmax(dim=-1)\n\n        self.spectrum_map = nn.Sequential(\n            nn.Conv1d(1, 10, 5, padding=\"same\").double(),\n            nn.LeakyReLU(),\n            nn.BatchNorm1d(10).double(),\n            nn.Conv1d(10, 5, 3, padding=\"same\").double(),\n            nn.LeakyReLU(),\n            nn.BatchNorm1d(5).double(),\n            nn.Conv1d(5, 1, 3, padding=\"same\").double(),\n        )\n        \n    def forward(self, x):\n        x = _atxa(self.bimap1, x)\n        evecs, eigs, _ = th.svd(x)\n        f_eigs = self.spectrum_map(eigs) # spectral map\n        v1 = _mvmt(self.P1, f_eigs, self.P1)            \n        v1 = self.manifold_SPD.proju(x, v1)\n        eigs = th.clamp(eigs, 1e-8, 1e8)\n        log_x = _mvmt(evecs, th.log(eigs), evecs)     \n        x = log_x + v1\n        if self.embed_only:\n            return x.reshape(x.shape[0],-1)\n        return self.softmax(self.fc_layer(x.reshape(x.shape[0],-1)))\n        #return self.softmax(self.classiflayer(x)) # Buseman Similarity Classifier when single modality is used\n            \nclass LogEucRResNet_Combo(nn.Module):\n    def __init__(self, inputdim_temp=36, dim1_temp=24, inputdim_spec=10, dim1_spec=7, n_proto=12, classes=11, buseman_classif=True):\n        super().__init__()\n        self.classes = classes\n        self.buseman_classif = buseman_classif\n        self.model_temp = LogEucRResNet(inputdim_temp, dim1_temp, n_proto, embed_only=True)\n        self.model_spectral = LogEucRResNet(inputdim_spec, dim1_spec, n_proto, embed_only=True)\n        \n        self.classiflayer = BusemanSimilarityClassifier(dim1_temp+1,n_proto,classes)\n        self.fc_layer = nn.Linear(dim1_temp*dim1_temp+dim1_spec*dim1_spec,classes).double()        \n        self.softmax = nn.Softmax(dim=-1)\n        \n    def forward(self, x1, x2):\n        out_1 = self.model_temp(x1) # probabilities\n        out_2 = self.model_spectral(x2) # probabilities\n        x = th.concat((out_1, out_2), dim=1)\n        #print(x.shape)\n        if self.buseman_classif:\n            return self.softmax(self.classiflayer(x))\n        else:\n            return self.softmax(self.fc_layer(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=LogEucRResNet().to(device)\n# model.load_state_dict(th.load('minitime_model.ckp'))\nprint(model.eval())\n\ntrainable_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(trainable_total_params)","metadata":{"_uuid":"613d02be-734f-4dce-b237-36da85217f6d","_cell_guid":"1a44b8ed-daa4-4cc9-8c3a-98bc7f6b0193","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-12T18:35:35.176967Z","iopub.execute_input":"2024-05-12T18:35:35.177264Z","iopub.status.idle":"2024-05-12T18:35:35.442709Z","shell.execute_reply.started":"2024-05-12T18:35:35.177241Z","shell.execute_reply":"2024-05-12T18:35:35.441760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"load dataset","metadata":{"_uuid":"ec6218d9-85f5-41d2-92c6-6e29fdb421cc","_cell_guid":"ff18b364-b139-4f41-b737-1c3fff1fb8e9","trusted":true}},{"cell_type":"code","source":"class TS_COV(Dataset):\n    def __init__(self, root_dir, np_file, estimator='scm'):\n        self.root_dir = root_dir\n        self.np_file = np_file\n        self.estimator = estimator\n        self.x, self.y, self.class_names = self.load_data()\n        #self.cov = normalize(self.calculate_covariances(),'corr')\n        self.x1 = self.calculate_covariances_36()\n        self.x2 = self.calculate_covariances_10()\n    \n    def __len__(self):\n        return len(self.x)\n\n    def __getitem__(self, idx):\n        x1, x2= self.x1[idx], self.x2[idx]\n        y_sample = self.y[idx]\n        class_name = self.class_names[y_sample]\n        x1 = th.from_numpy(np.asarray([x1])).double()\n        x2 = th.from_numpy(np.asarray([x2])).double()\n        x1,x2 = x1.reshape(1, 36, 36), x2.reshape(1, 10, 10)\n        y = th.from_numpy(np.array(y_sample)).long()\n        return x1,x2, y#, class_name\n\n    def load_data(self):\n        file_path = os.path.join(self.root_dir, self.np_file)\n        data = np.load(file_path, allow_pickle= True)\n        x = data['series']\n        y = data['labels']\n        class_names = data['class_names']\n        return x, y, class_names\n\n    def calculate_covariances_10(self):\n        x_transposed = np.transpose(self.x, (0, 2, 1))\n        cov = covariances(x_transposed, estimator=self.estimator)\n        return cov\n    def calculate_covariances_36(self):\n        cov = covariances(self.x, estimator=self.estimator)\n        return cov\n        \n","metadata":{"_uuid":"9b047b2a-9ae2-438a-9b64-2797d6bc47ec","_cell_guid":"d43c9a4b-6519-4157-bfe7-f80679cddcf9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-12T18:35:35.463960Z","iopub.execute_input":"2024-05-12T18:35:35.464250Z","iopub.status.idle":"2024-05-12T18:35:35.474438Z","shell.execute_reply.started":"2024-05-12T18:35:35.464226Z","shell.execute_reply":"2024-05-12T18:35:35.473678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_dir = r\"/kaggle/input/pre-processed-data/\"\n# root_dir = r'/kaggle/input/normalized-data/'\n# npz_file = \"98_p_norm_data.npz\"\nnpz_file= \"data_preprocessed.npz\"\nbatch_size=64 #batch size\ndef calculate_class_weights(labels):\n    class_sample_count = np.array([len(np.where(labels == t)[0]) for t in np.unique(labels)])\n    weight = 1 - class_sample_count / len(labels)\n    return th.from_numpy(weight).double()\n\ndef create_data_loaders(dataset, train_indices, val_indices, test_indices, batch_size):\n    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_indices))\n    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(val_indices))\n    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(test_indices), shuffle=False)\n    return train_loader, val_loader, test_loader\n\ndef plot_class_distribution(train_loader, val_loader, test_loader, n_classes):\n    train_counts = np.zeros(n_classes)\n    for _, _, labels in train_loader:\n        for l in labels:\n            train_counts[l] += 1\n\n    val_counts = np.zeros(n_classes)\n    for _, _, labels in val_loader:\n        for l in labels:\n            val_counts[l] += 1\n\n    test_counts = np.zeros(n_classes)\n    for _, _, labels in test_loader:\n        for l in labels:\n            test_counts[l] += 1\n\n    plt.figure(figsize=(12, 6))\n    plt.bar(np.arange(n_classes), train_counts, alpha=0.5, label='Train')\n    plt.bar(np.arange(n_classes), val_counts, alpha=0.5, label='Validation')\n    plt.bar(np.arange(n_classes), test_counts, alpha=0.5, label='Test')\n    plt.xlabel('Classes')\n    plt.ylabel('Count')\n    plt.yscale('log')\n    plt.title('Class Distribution in Train, Validation, and Test Sets')\n    plt.legend(loc='best')\n    plt.show()\n\n# Initialize dataset\ndataset = TS_COV(root_dir=root_dir, np_file=npz_file, estimator='scm')\n\n# Split data into train, validation, and test sets\ntrain_indices, test_indices = train_test_split(list(range(len(dataset))), test_size=0.2, stratify=dataset.y, random_state=42)\ntrain_indices, val_indices = train_test_split(train_indices, test_size=0.2, stratify=dataset.y[train_indices], random_state=42)\n\n# Calculate class weights\nweights = calculate_class_weights(dataset.y)\n\n# Create data loaders\ntrain_loader, val_loader, test_loader = create_data_loaders(dataset, train_indices, val_indices, test_indices, batch_size)\n\n# Plot class distribution\nplot_class_distribution(train_loader, val_loader, test_loader, n_classes=11)\n","metadata":{"_uuid":"88eef8dc-326f-42f0-8cad-508720d6d94b","_cell_guid":"f15a0836-289c-40dd-8555-380ab65362b4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-12T18:35:35.475593Z","iopub.execute_input":"2024-05-12T18:35:35.476018Z","iopub.status.idle":"2024-05-12T18:35:48.578543Z","shell.execute_reply.started":"2024-05-12T18:35:35.475987Z","shell.execute_reply":"2024-05-12T18:35:48.577634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# access one matri\nfor x1,x2, y in train_loader:\n    print(x1.shape)\n    print(x2.shape)\n    print(y.shape)\n    break","metadata":{"_uuid":"3b68e633-fb1f-419c-8197-7e3afdfc3ff1","_cell_guid":"0c29a127-413b-4983-87b6-55f1205a381c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-12T18:35:48.579957Z","iopub.execute_input":"2024-05-12T18:35:48.580600Z","iopub.status.idle":"2024-05-12T18:35:49.175160Z","shell.execute_reply.started":"2024-05-12T18:35:48.580563Z","shell.execute_reply":"2024-05-12T18:35:49.174246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr=1e-2\nepochs=200\ngamma_value=0.5\nstep_size=30\n#setup loss and optimizer\n#loss_fn = nn.CrossEntropyLoss()\nloss_fn = FocalLoss(gamma=3,weights=weights)\n#opti = MixOptimizer(model.parameters(),lr=lr)\nopti = geoopt.optim.RiemannianAdam(model.parameters(), lr=lr)\nscheduler = lr_scheduler.ExponentialLR(opti, gamma=gamma_value)\n#scheduler = th.optim.lr_scheduler.StepLR(opti, step_size, gamma_value)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_loader,val_loader loss_fn, optimizer, scheduler, epochs):\n    t1, t2, t3, t4, t5 = [], [], [], [], []\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    for epoch in range(epochs):\n        start_time = time.time()  # Start time of the epoch\n        model.train()\n\n        # Training phase\n        loss_train, acc_train = [], []\n        for x1,x2,y in train_loader:\n            x1 = x1.to(device)\n            x2 = x2.to(device)\n            y = y.to(device).long()\n            optimizer.zero_grad()\n            out = model(x1,x2)\n            l = loss_fn(out, y)\n            \n            acc, loss = (out.argmax(1)==y).cpu().numpy().sum()/out.shape[0], l.cpu().data.numpy()\n            loss_train.append(loss)\n            acc_train.append(acc)\n            l.backward()\n            optimizer.step()\n        scheduler.step()\n\n        acc_train = np.asarray(acc_train).mean()\n        loss_train = np.asarray(loss_train).mean()\n        t1.append(loss_train)\n        t2.append(100*acc_train)\n\n        # Validation phase\n        loss_val, acc_val_list = [], []\n        y_true, y_pred = [], []\n        model.eval()\n        for x1,x2, y in val_loader:\n            x1 = x1.to(device)\n            x2 = x2.to(device)\n            y = y.to(device).long()\n            with torch.no_grad():\n                out = model(x1,x2)\n                l = loss_fn(out, y)\n                \n            loss_val.append(l.cpu().data.numpy())\n            predicted_labels = out.argmax(1)\n            y_true.extend(list(y.cpu().numpy()))\n            y_pred.extend(list(predicted_labels.cpu().numpy()))\n            acc = (predicted_labels==y).cpu().numpy().sum()/out.shape[0]\n            \n            acc_val_list.append(acc)\n        acc_val = np.asarray(acc_val_list).mean()\n        loss_val = np.asarray(loss_val).mean()    \n   \n        t3.append(100 * acc_val)\n        t4.append(loss_val)\n        f1_test = f1_score(y_true, y_pred, average='macro')\n        t5.append(100 * f1_test)\n        print('Epoch {}/{} - Train loss: {:.4f} - Val loss: {:.4f} - Train acc: {:.2f}% - Val acc: {:.2f}% - Test F1-score: {:.2f}'.format(\n            epoch + 1, epochs, loss_train, loss_val, 100 * acc_train, 100 * acc_val, 100 * f1_test))\n\n        elapsed_time = time.time() - start_time\n        print('Elapsed time: {:.2f} seconds'.format(elapsed_time))\n\n    return t1, t2, t3, t4, t5\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T18:35:49.176434Z","iopub.execute_input":"2024-05-12T18:35:49.176746Z","iopub.status.idle":"2024-05-12T18:35:49.191812Z","shell.execute_reply.started":"2024-05-12T18:35:49.176719Z","shell.execute_reply":"2024-05-12T18:35:49.190898Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ndef evaluate_model(model, data_loader, loss_fn, model_name, figure_name):\n    loss_val, acc_val = [], []\n    y_true, y_pred = [], []\n\n    model.eval()\n    for x1, x2, y in data_loader:\n        x1 = x1.to(device)\n        x2 = x2.to(device)\n        y = y.to(device).long()\n        with torch.no_grad():\n            out = model(x1, x2)\n            l = loss_fn(out, y)\n        predicted_labels = out.argmax(1)\n        y_true.extend(list(y.cpu().numpy()))\n        y_pred.extend(list(predicted_labels.cpu().numpy()))\n        acc = (predicted_labels == y).cpu().numpy().sum() / out.shape[0]\n        loss_val.append(l.cpu().data.numpy())\n        acc_val.append(acc)\n\n    acc_val = np.asarray(acc_val).mean()\n    loss_val = np.asarray(loss_val).mean()\n\n    print('Validation loss: {:.4f}'.format(loss_val))\n    print('Validation accuracy: {:.2f}%'.format(100 * acc_val))\n\n    # Save the model\n    model_save_path = f'/kaggle/working/{model_name}.ckpt'\n    torch.save(model.state_dict(), model_save_path)\n    print(f'Model saved to {model_save_path}')\n\n    # Print classification report\n    print(classification_report(y_true, y_pred, digits=2, target_names=data_loader.dataset.class_names))\n    cmatrix = confusion_matrix(y_true, y_pred)\n    Ncmatrix = cmatrix.astype('float') / cmatrix.sum(axis=1)[:, np.newaxis]\n\n    group_counts = ['{0:0.0f}'.format(value) for value in cmatrix.flatten()]\n    group_percentages = ['{0:.2%}'.format(value) for value in Ncmatrix.flatten()]\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_counts, group_percentages)]\n    labels = np.asarray(labels).reshape(11, 11)\n\n    # Plot and save the confusion matrix figure\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(Ncmatrix, annot=labels, fmt='', cmap='Reds', xticklabels=data_loader.dataset.class_names, yticklabels=data_loader.dataset.class_names)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\n Acc={:.2f}%'.format(100 * acc_val))\n    figure_save_path = f'/kaggle/working/{figure_name}.png'\n    plt.savefig(figure_save_path)\n    plt.show()\n    print(f'Figure saved to {figure_save_path}')\n\n    return 100 * acc_val\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T19:19:53.630078Z","iopub.status.idle":"2024-05-12T19:19:53.630600Z","shell.execute_reply.started":"2024-05-12T19:19:53.630330Z","shell.execute_reply":"2024-05-12T19:19:53.630353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(model, data_loader, loss_fn, model_name_params, figure_name_model_name_params)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T19:19:53.632745Z","iopub.status.idle":"2024-05-12T19:19:53.633149Z","shell.execute_reply.started":"2024-05-12T19:19:53.632957Z","shell.execute_reply":"2024-05-12T19:19:53.632974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compute some TSNE visualization of the embedding","metadata":{"_uuid":"a6e243f1-b4b9-4598-9b62-a6a0eab6f822","_cell_guid":"97dbd4d8-6f69-4c3b-adbf-d1a414b0fc42","trusted":true}},{"cell_type":"code","source":"\ndef gen_features(net,dataloader):\n    net.eval()\n    targets_list = []\n    outputs_list = []\n\n    with th.no_grad():\n        for idx, (inputs, targets) in enumerate(dataloader):\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            targets_np = targets.data.cpu().numpy()\n\n            outputs = net.partial_forward(inputs)\n            outputs_np = outputs.data.cpu().numpy()\n            \n            targets_list.append(targets_np[:, np.newaxis])\n            outputs_list.append(outputs_np)\n            \n            if ((idx+1) % 10 == 0) or (idx+1 == len(dataloader)):\n                print(idx+1, '/', len(dataloader))\n\n    targets = np.concatenate(targets_list, axis=0)\n    outputs = np.concatenate(outputs_list, axis=0).astype(np.float64)\n\n    return targets, outputs\n\ndef tsne_plot(save_dir, targets, outputs):\n    print('generating t-SNE plot...')\n    # tsne_output = bh_sne(outputs)\n    tsne = TSNE(random_state=0, perplexity=50)\n    tsne_output = tsne.fit_transform(outputs)\n\n    df = pd.DataFrame(tsne_output, columns=['x', 'y'])\n    df['targets'] = targets\n\n    plt.rcParams['figure.figsize'] = 10, 10\n    sns.scatterplot(\n        x='x', y='y',\n        hue='targets',\n        palette=sns.color_palette(\"Paired\", 11),\n        data=df,\n        marker='o',\n        legend=\"full\",\n        alpha=0.5\n    )\n\n    plt.xticks([])\n    plt.yticks([])\n    plt.xlabel('')\n    plt.ylabel('')\n\n    #plt.savefig(os.path.join(save_dir,'tsne.png'), bbox_inches='tight')\n    print('done!')","metadata":{"_uuid":"30a2288e-55e6-4cf2-99cd-a9a00d8d1a5f","_cell_guid":"6f6b7804-c74c-426f-a91c-166d46b4a5d3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-12T19:19:53.634576Z","iopub.status.idle":"2024-05-12T19:19:53.634970Z","shell.execute_reply.started":"2024-05-12T19:19:53.634789Z","shell.execute_reply":"2024-05-12T19:19:53.634805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#output_dir = r'/kaggle/working/'\n\ntargets, outputs = gen_features(model,data_loader._test_generator)\ntsne_plot('.', targets, outputs)","metadata":{"_uuid":"2d94ee40-916b-4a2f-9520-fe42d6496084","_cell_guid":"6be9ffa9-9b34-4b73-8cdd-e387116d66b2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-12T19:19:53.636267Z","iopub.status.idle":"2024-05-12T19:19:53.636696Z","shell.execute_reply.started":"2024-05-12T19:19:53.636463Z","shell.execute_reply":"2024-05-12T19:19:53.636480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"aab08407-1394-4f26-9856-aa9c4abfdc8c","_cell_guid":"e5a72ebd-8150-4252-aefd-f96b7541b71a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}